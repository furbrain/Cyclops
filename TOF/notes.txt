So I've ordered a TOF camera from the Pi Hut - and also a low light camera as well

General plan

We have both cameras and the IMU contributing to a single video file - need to do some gnarly
gstreamer stuff.

General 3d reconstruction pipeline:

Generate trajectory from IMU data - should have reasonably good pose estimation, but
may have bad translation errors. Use this with TOF data to produce a low-res model of the cave.
Can also do a first correction on the trajectory at this point.
we can possibly do the bit up to here on pi zero to allow sanity check before leaving cave


We can then use the visible camera images to get further away pixel data and "fill in the blanks",
may include a further trajectory correction. This bit would be just using feature matching again,
so still fairly low res model.

Finally can do higher res dense model using sparse model and (hopefully now quite accurate)
trajectory


Calibration requirements: IMU needs calibrating. Need registration between TOF and low light
cameras


Challenges: Can we just stream proper raw data to the video file?
Can we then process that raw data later and extract depth and confidence info. Alternatively,
if we can just stream pure depth data, would that be satisfactory

Ok, looks like we can do this - need to use the arducamdevice in c++ - no python binding

How fast would motion collation be on pi zero - could we process on phone instead???
Could decimate depth images before processing on pi zero - lower quality and risk of more gaps...


Power: TOF cam uses 3.5W, low light cam uses 1.5W pizero2 uses ... some. COuld possibly do with
formal testing and mybe just a bigger battery; could still use with waveshare board though...


Tasks:

* make gstreamer raw grayscale source - doesn't like GRAY12 or Gray16_LE
* got encoding raw to I420 format working
* use hardware h264 encoding - loses data in low bits. does not seem to do lossless on v4l2
* working going in to matroska - can decode coming out nicely
* next step is to get (fake)I420 -> GRAY16_LE working - test visually via opencv on laptop
* then need to get ArducamDevice processing working...

Notes:

needs numpy < 2.0.0 in .venv for arducam python stuff

device type is TOFDeviceType.HQVGA: 1
RAW: 720 height 240 width type TOFOutput.RAW #note should be 180,960 shape
DEPTH : 180 height 240 width type TOFOutput.DEPTH


cache -> raw
abcd -> eab0  c is discarded d is transformed to e according to 0123|89ab -> 0123 4567|cdef -> cdef
(or have I confused myself with endiannnessss nonsensseeee)
Cache is 240*180*4*2, but last 480 bytes are unused

Raw is 960*180
First 720 are blank, as are last 240

Next job is to create dummy sensor to open...